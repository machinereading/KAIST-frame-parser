{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from tqdm import tqdm, trange\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "from src import dataio\n",
    "from src.fn_modeling import BertForFrameIdentification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SETINGS\n",
      "\t# FrameNet: Korean FrameNet 1.0\n",
      "\t# model will be saved to ./models/kfn/\n",
      "\t# result will be saved to ./result/\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "batch_size = 8\n",
    "language = 'ko'\n",
    "version = 1.0\n",
    "\n",
    "if language == 'en':\n",
    "    framenet = 'fn'\n",
    "    framenet_data = 'English FrameNet '+str(version)\n",
    "elif language == 'ko':\n",
    "    framenet = 'kfn'\n",
    "    framenet_data = 'Korean FrameNet '+str(version)\n",
    "\n",
    "# save your model to\n",
    "model_dir = './models/'+framenet+'/'\n",
    "result_dir = './result/'\n",
    "\n",
    "print('### SETINGS')\n",
    "print('\\t# FrameNet:', framenet_data)\n",
    "print('\\t# model will be saved to', model_dir)\n",
    "print('\\t# result will be saved to', result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Korean FrameNet ###\n",
      "\t# contact: hahmyg@kaist, hahmyg@gmail.com #\n",
      "\n",
      "\n",
      "### loading Korean FrameNet 1.0 data...\n",
      "\t# of instances in training data: 12431\n",
      "\t# of instances in dev data: 624\n",
      "\t# of instances in test data: 4382\n",
      "\n",
      "an example of dataset\n",
      "[['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '순이익이', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'B-Time', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "from koreanframenet import koreanframenet\n",
    "if language == 'ko':\n",
    "    kfn = koreanframenet.interface(version)\n",
    "    trn, dev, tst = kfn.load_data()\n",
    "    \n",
    "# print('\\nan example of dataset')\n",
    "# print(trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Statistics...\n",
      "\t# of lu: 4065\n"
     ]
    }
   ],
   "source": [
    "data_path = './koreanframenet/resource/info/'\n",
    "\n",
    "with open(data_path+framenet+str(version)+'_lu2idx.json','r') as f:\n",
    "    lu2idx = json.load(f)\n",
    "with open(data_path+'fn1.7_frame2idx.json','r') as f:\n",
    "    sense2idx = json.load(f)      \n",
    "with open(data_path+framenet+str(version)+'_lufrmap.json','r') as f:\n",
    "    lusensemap = json.load(f)\n",
    "    \n",
    "idx2sense = dict(zip(sense2idx.values(),sense2idx.keys()))\n",
    "idx2lu = dict(zip(lu2idx.values(),lu2idx.keys()))\n",
    "        \n",
    "print('\\nData Statistics...')\n",
    "print('\\t# of lu:', len(lu2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD BERT TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "# bert tokenizer\n",
    "def bert_tokenizer(text):\n",
    "    orig_tokens = text.split(' ')\n",
    "    bert_tokens = []\n",
    "    orig_to_tok_map = []\n",
    "    bert_tokens.append(\"[CLS]\")\n",
    "    for orig_token in orig_tokens:\n",
    "        orig_to_tok_map.append(len(bert_tokens))\n",
    "        bert_tokens.extend(tokenizer.tokenize(orig_token))\n",
    "    bert_tokens.append(\"[SEP]\")\n",
    "    \n",
    "    return orig_tokens, bert_tokens, orig_to_tok_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE BERT input representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(input_data):\n",
    "    tokenized_texts, lus, senses = [],[],[]\n",
    "\n",
    "    for i in range(len(input_data)):    \n",
    "        data = input_data[i]\n",
    "        text = ' '.join(data[0])\n",
    "        orig_tokens, bert_tokens, orig_to_tok_map = bert_tokenizer(text)\n",
    "        tokenized_texts.append(bert_tokens)\n",
    "\n",
    "        ori_lus = data[1]    \n",
    "        lu_sequence = []\n",
    "        for i in range(len(bert_tokens)):\n",
    "            if i in orig_to_tok_map:\n",
    "                idx = orig_to_tok_map.index(i)\n",
    "                l = ori_lus[idx]\n",
    "                lu_sequence.append(l)\n",
    "            else:\n",
    "                lu_sequence.append('_')\n",
    "        lus.append(lu_sequence)        \n",
    "        \n",
    "        ori_senses = data[2]    \n",
    "        sense_sequence = []\n",
    "        for i in range(len(bert_tokens)):\n",
    "            if i in orig_to_tok_map:\n",
    "                idx = orig_to_tok_map.index(i)\n",
    "                l = ori_senses[idx]\n",
    "                sense_sequence.append(l)\n",
    "            else:\n",
    "                sense_sequence.append('_')\n",
    "        senses.append(sense_sequence)\n",
    "\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    tgt_seq, lu_seq, sense_seq = [],[],[]\n",
    "    for sent_idx in range(len(lus)):\n",
    "        lu_items = lus[sent_idx]\n",
    "        sense_items = senses[sent_idx]\n",
    "        tgt,lu, sense = [],[],[]\n",
    "        for idx in range(len(lu_items)):\n",
    "            if lu_items[idx] != '_':\n",
    "                if len(tgt) == 0:\n",
    "                    tgt.append(idx)\n",
    "                    lu.append(lu2idx[lu_items[idx]])\n",
    "        for idx in range(len(sense_items)):\n",
    "            if sense_items[idx] != '_':\n",
    "                if len(sense) == 0:\n",
    "                    sense.append(sense2idx[sense_items[idx]])\n",
    "        tgt_seq.append(tgt)\n",
    "        lu_seq.append(lu)\n",
    "        sense_seq.append(sense)\n",
    "        \n",
    "    attention_masks = [[float(i>0) for i in ii] for ii in input_ids]    \n",
    "    data_inputs = torch.tensor(input_ids)\n",
    "    data_tgt_idx = torch.tensor(tgt_seq)\n",
    "    data_lus = torch.tensor(lu_seq)\n",
    "    data_senses = torch.tensor(sense_seq)\n",
    "    data_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return data_inputs, data_tgt_idx, data_lus, data_senses, data_masks\n",
    "\n",
    "trn_inputs, trn_tgt_idxs, trn_lus, trn_senses, trn_masks = gen_data(trn)\n",
    "dev_inputs, dev_tgt_idxs, dev_lus, dev_senses, dev_masks = gen_data(dev)\n",
    "tst_inputs, tst_tgt_idxs, tst_lus, tst_senses, tst_masks = gen_data(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = TensorDataset(trn_inputs, trn_tgt_idxs, trn_lus, trn_senses, trn_masks)\n",
    "trn_sampler = RandomSampler(trn_data)\n",
    "trn_dataloader = DataLoader(trn_data, sampler=trn_sampler, batch_size=batch_size)\n",
    "\n",
    "dev_data = TensorDataset(dev_inputs, dev_tgt_idxs, dev_lus, dev_senses, dev_masks)\n",
    "dev_sampler = RandomSampler(dev_data)\n",
    "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "tst_data = TensorDataset(tst_inputs, tst_tgt_idxs, tst_lus, tst_senses, tst_masks)\n",
    "tst_sampler = RandomSampler(tst_data)\n",
    "tst_dataloader = DataLoader(tst_data, sampler=tst_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD BERT framenet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForFrameIdentification.from_pretrained(\"bert-base-multilingual-cased\", num_labels = len(sense2idx), num_lus = len(lu2idx), ludim = 64, lusensemap=lusensemap)\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING the pretrained BERT language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():    \n",
    "    epochs = 5\n",
    "    max_grad_norm = 1.0\n",
    "    num_of_epoch = 0\n",
    "    accuracy_result = []\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        # TRAIN loop\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            # add batch to gpu\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_tgt_idxs, b_input_lus, b_input_senses, b_input_masks = batch            \n",
    "            # forward pass\n",
    "            loss = model(b_input_ids, token_type_ids=None, tgt_idxs=b_input_tgt_idxs, \n",
    "                         lus=b_input_lus, senses=b_input_senses, attention_mask=b_input_masks)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        # print train loss per epoch\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        num_of_epoch += 1\n",
    "        model_path = model_dir+'frame_identifier-epoch-'+str(num_of_epoch)+'.pt'\n",
    "        torch.save(model, model_path)        \n",
    "\n",
    "        # evaluation for each epoch\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        predictions , true_labels, scores, candis, all_lus = [], [], [], [], []\n",
    "        for batch in tst_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_tgt_idxs, b_lus, b_senses, b_masks = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_eval_loss = model(b_input_ids, token_type_ids=None, tgt_idxs=b_tgt_idxs, \n",
    "                                     lus=b_lus, senses=b_senses, attention_mask=b_masks)\n",
    "                logits = model(b_input_ids, token_type_ids=None, tgt_idxs=b_tgt_idxs, \n",
    "                                lus=b_lus, attention_mask=b_masks)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_senses.to('cpu').numpy()          \n",
    "            masks = dataio.get_masks(b_lus, lusensemap, num_label=len(sense2idx)).to(device)\n",
    "            for lu in b_lus:\n",
    "                candi_idx = lusensemap[str(int(lu))]\n",
    "                candi = [idx2sense[c] for c in candi_idx]\n",
    "                candi_txt = ','.join(candi)\n",
    "                candi_txt = str(len(candi))+'\\t'+candi_txt\n",
    "                candis.append(candi_txt)\n",
    "                all_lus.append(idx2lu[int(lu)])\n",
    "            \n",
    "            for b_idx in range(len(logits)):\n",
    "                logit = logits[b_idx]\n",
    "                mask = masks[b_idx]\n",
    "                b_pred_idxs, b_pred_logits = [],[]\n",
    "                for fr_idx in range(len(mask)):\n",
    "                    if mask[fr_idx] > 0:\n",
    "                        b_pred_idxs.append(fr_idx)\n",
    "                        b_pred_logits.append(logit[0][fr_idx].item())\n",
    "                b_pred_idxs = torch.tensor(b_pred_idxs)\n",
    "                b_pred_logits = torch.tensor(b_pred_logits)\n",
    "                sm = nn.Softmax()\n",
    "                b_pred_logits = sm(b_pred_logits).view(1, -1)\n",
    "                score, indice = b_pred_logits.max(1)                \n",
    "                prediction = b_pred_idxs[indice]\n",
    "                predictions.append([int(prediction)])\n",
    "                score = float(score)\n",
    "                scores.append(score)\n",
    "            true_labels.append(label_ids)\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "        eval_loss = eval_loss/nb_eval_steps\n",
    "        print(\"Validation loss: {}\".format(eval_loss))\n",
    "        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "        pred_tags = [idx2sense[p_i] for p in predictions for p_i in p]\n",
    "        valid_tags = [idx2sense[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "        acc = accuracy_score(pred_tags, valid_tags)\n",
    "        accuracy_result.append(acc)\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "        \n",
    "        result_path = result_dir+'frameid-'+str(num_of_epoch)+'.tsv'\n",
    "        with open(result_path,'w') as f:\n",
    "            line = 'gold' + '\\t' + 'prediction' + '\\t' + 'score' + '\\t' + 'input_lu' + '\\t' + 'sense_candidates'\n",
    "            f.write(line+'\\n')\n",
    "            for item in range(len(pred_tags)):\n",
    "                line = valid_tags[item] + '\\t' + pred_tags[item] + '\\t' + str(scores[item]) +'\\t'+ all_lus[item]+'\\t' + candis[item]\n",
    "                f.write(line+'\\n')\n",
    "    accuracy_result_path = result_dir+'frameid.accuracy'\n",
    "    with open(accuracy_result_path,'w') as f:\n",
    "        n = 0\n",
    "        for acc in accuracy_result:\n",
    "            f.write('epoch:'+str(n)+'\\t' + 'accuracy: '+str(acc)+'\\n')\n",
    "            n +=1\n",
    "\n",
    "# training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_identifier(bert_inputs):\n",
    "    data_inputs, data_tgt_idx, data_lus, data_senses, data_masks = bert_inputs[0],bert_inputs[1],bert_inputs[2],bert_inputs[3],bert_inputs[4]\n",
    "    input_data = TensorDataset(data_inputs, data_tgt_idx, data_lus, data_senses, data_masks)\n",
    "#     trn_sampler = RandomSampler(trn_data)\n",
    "    trn_dataloader = DataLoader(trn_data, sampler=None, batch_size=batch_size)\n",
    "    return trn_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
