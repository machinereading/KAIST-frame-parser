{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from tqdm import tqdm, trange\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "from src import dataio\n",
    "from src.fn_modeling import BertForFrameIdentification\n",
    "from src.fn_modeling import BertForArgClassification\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SETINGS\n",
      "\t# FrameNet: Korean FrameNet 1.1\n",
      "\t# model will be saved to ./models/kfn/\n",
      "\t# result will be saved to ./result/\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "batch_size = 8\n",
    "language = 'ko'\n",
    "version = 1.1\n",
    "\n",
    "global_frargmap = False\n",
    "\n",
    "if global_frargmap == True:\n",
    "    frarg_type = 'global-frargmap-'\n",
    "else:\n",
    "    frarg_type = 'local-frargmap-'\n",
    "\n",
    "framenet = 'kfn'\n",
    "framenet_data = 'Korean FrameNet '+str(version)\n",
    "\n",
    "# save your model to\n",
    "model_dir = './models/'+framenet+'/'\n",
    "result_dir = './result/'\n",
    "\n",
    "print('### SETINGS')\n",
    "print('\\t# FrameNet:', framenet_data)\n",
    "print('\\t# model will be saved to', model_dir)\n",
    "print('\\t# result will be saved to', result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Korean FrameNet ###\n",
      "\t# contact: hahmyg@kaist, hahmyg@gmail.com #\n",
      "\n",
      "\n",
      "### loading Korean FrameNet 1.1 data...\n",
      "\t# of instances in training data: 17838\n",
      "\t# of instances in dev data: 2548\n",
      "\t# of instances in test data: 5097\n"
     ]
    }
   ],
   "source": [
    "from koreanframenet import koreanframenet\n",
    "if language == 'ko':\n",
    "    kfn = koreanframenet.interface(version=version)\n",
    "    trn, dev, tst = kfn.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gen arg-granularity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of examples in trn: 28536\n",
      "# of examples in dev: 4111\n",
      "# of examples in tst: 8083\n"
     ]
    }
   ],
   "source": [
    "def data2argdata(data):\n",
    "    result = []\n",
    "    for i in data:\n",
    "        tokens, lus, frames, args = i[0],i[1],i[2],i[3]\n",
    "        for idx in range(len(args)):\n",
    "            arg_tag = args[idx]\n",
    "            if arg_tag.startswith('B'):\n",
    "                new_args = ['O' for i in range(len(tokens))]                \n",
    "                fe_tag = arg_tag.split('-')[1]\n",
    "                next_idx = idx + 1\n",
    "                while next_idx < len(args) and args[next_idx] == 'I-'+fe_tag:\n",
    "                    next_idx +=1\n",
    "                new_args[next_idx-1] = fe_tag\n",
    "                new_sent = []\n",
    "                new_sent.append(tokens)\n",
    "                new_sent.append(lus)\n",
    "                new_sent.append(frames)\n",
    "                new_sent.append(new_args)\n",
    "                result.append(new_sent)\n",
    "    return result\n",
    "    \n",
    "trn = data2argdata(trn)\n",
    "dev = data2argdata(dev)\n",
    "tst = data2argdata(tst)\n",
    "print('# of examples in trn:', len(trn))\n",
    "print('# of examples in dev:', len(dev))\n",
    "print('# of examples in tst:', len(tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdata example\n",
      "\ttrn[0]\n",
      "[['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '순이익이', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Earner', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "\n",
      "\ttrn[0]\n",
      "[['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '순이익이', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Time', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print('\\tdata example')\n",
    "print('\\ttrn[0]')\n",
    "print(trn[0])\n",
    "print('\\n\\ttrn[0]')\n",
    "print(trn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Statistics...\n",
      "\t# of lu: 5489\n"
     ]
    }
   ],
   "source": [
    "data_path = './koreanframenet/resource/info/'\n",
    "\n",
    "with open(data_path+framenet+str(version)+'_lu2idx.json','r') as f:\n",
    "    lu2idx = json.load(f)\n",
    "with open(data_path+'fn1.7_frame2idx.json','r') as f:\n",
    "    frame2idx = json.load(f)\n",
    "with open(data_path+'fn1.7_fe2idx.json','r') as f:\n",
    "    arg2idx = json.load(f)\n",
    "with open(data_path+framenet+str(version)+'_lufrmap.json','r') as f:\n",
    "    lufrmap = json.load(f)\n",
    "    \n",
    "if global_frargmap == True:\n",
    "    with open(data_path+'fn1.7_frargmap.json','r') as f:\n",
    "        frargmap = json.load(f)\n",
    "else:\n",
    "    with open(data_path+'kfn1.1_frargmap.json','r') as f:\n",
    "        frargmap = json.load(f)\n",
    "    \n",
    "idx2frame = dict(zip(frame2idx.values(),frame2idx.keys()))\n",
    "idx2lu = dict(zip(lu2idx.values(),lu2idx.keys()))\n",
    "idx2arg = dict(zip(arg2idx.values(),arg2idx.keys()))\n",
    "        \n",
    "print('\\nData Statistics...')\n",
    "print('\\t# of lu:', len(lu2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pretrained BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "# # bert tokenizer\n",
    "# def bert_tokenizer(text):\n",
    "#     orig_tokens = text.split(' ')\n",
    "#     bert_tokens = []\n",
    "#     orig_to_tok_map = []\n",
    "#     bert_tokens.append(\"[CLS]\")\n",
    "#     for orig_token in orig_tokens:\n",
    "#         orig_to_tok_map.append(len(bert_tokens))\n",
    "#         bert_tokens.extend(tokenizer.tokenize(orig_token))\n",
    "#     bert_tokens.append(\"[SEP]\")\n",
    "    \n",
    "#     return orig_tokens, bert_tokens, orig_to_tok_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BERT input representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate BERT input representation ...\n",
      "... is done\n"
     ]
    }
   ],
   "source": [
    "print('generate BERT input representation ...')\n",
    "bert_io = dataio.for_BERT(mode='training', version=version)\n",
    "\n",
    "trn_data = bert_io.convert_to_bert_input_arg_classifier(trn)\n",
    "trn_sampler = RandomSampler(trn_data)\n",
    "trn_dataloader = DataLoader(trn_data, sampler=trn_sampler, batch_size=batch_size)\n",
    "\n",
    "dev_data = bert_io.convert_to_bert_input_arg_classifier(dev)\n",
    "dev_sampler = RandomSampler(dev_data)\n",
    "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "tst_data = bert_io.convert_to_bert_input_arg_classifier(tst)\n",
    "tst_sampler = RandomSampler(tst_data)\n",
    "tst_dataloader = DataLoader(tst_data, sampler=tst_sampler, batch_size=batch_size)\n",
    "print('... is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['재해', '손실은', 'Aetna의', '순이익을', 'Hugo로', '인한', '36', '백만', '달러를', '포함하여,', '50', '백만', '달러로', '감소시켰다.'], ['_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'Earner', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "\n",
      "trn_inputs\n",
      "tensor([   101,   9659,  14523,   9450,  31503,  10892,    138,  10308,  10219,\n",
      "         10459,   9462,  10739, 119188,  10622,  15945,  11261,   9640,  11102,\n",
      "         11055,   9331,  19105,   9061,  30873,  11513,   9928,  48533,  13374,\n",
      "           117,  10462,   9331,  19105,   9061,  30873,  11261,   8848,  22333,\n",
      "         14040,  84513,    119,    102,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0])\n",
      "\n",
      "tgt_idxs\n",
      "tensor([13])\n",
      "\n",
      "lus\n",
      "tensor([3675])\n",
      "\n",
      "frames\n",
      "tensor([397])\n",
      "Earnings_and_losses\n",
      "\n",
      "arg_idxs\n",
      "tensor([9])\n",
      "\n",
      "args\n",
      "tensor([354])\n",
      "Earner\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # print(len(trn_data[0]))\n",
    "# idx = 3\n",
    "# trn_inputs, trn_tgt_idxs, trn_lus, trn_frames, trn_arg_idxs, trn_args, trn_masks = trn_data[idx]\n",
    "\n",
    "\n",
    "# print(trn[idx])\n",
    "# print('\\ntrn_inputs')\n",
    "# print(trn_inputs)\n",
    "# print('\\ntgt_idxs')\n",
    "# print(trn_tgt_idxs)\n",
    "# print('\\nlus')\n",
    "# print(trn_lus)\n",
    "# print('\\nframes')\n",
    "# print(trn_frames)\n",
    "# print(idx2frame[int(trn_frames[0])])\n",
    "# print('\\narg_idxs')\n",
    "# print(trn_arg_idxs)\n",
    "# print('\\nargs')\n",
    "# print(trn_args)\n",
    "# print(idx2arg[int(trn_args[0])])\n",
    "# print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load BERT arg-classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForArgClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels = len(arg2idx), num_lus = len(lu2idx), num_frames = len(frame2idx), ludim = 64, framedim = 100, frargmap=frargmap)\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING the pretrained BERT LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 7.187800407409668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\r",
      "Epoch:  14%|█▍        | 1/7 [00:03<00:23,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -0.012521723285317421\n",
      "Validation Accuracy: 0.0\n",
      "Accuracy: 0.125\n",
      "Train loss: 7.117981433868408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  29%|██▊       | 2/7 [00:08<00:20,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -0.008522172458469868\n",
      "Validation Accuracy: 0.0\n",
      "Accuracy: 0.0\n",
      "Train loss: 7.135272979736328\n",
      "Validation loss: -0.019932890310883522\n",
      "Validation Accuracy: 0.0\n",
      "Accuracy: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  43%|████▎     | 3/7 [00:12<00:16,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 7.078927516937256\n",
      "Validation loss: -0.025791414082050323\n",
      "Validation Accuracy: 0.25\n",
      "Accuracy: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  57%|█████▋    | 4/7 [00:16<00:12,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 6.909806251525879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  71%|███████▏  | 5/7 [00:20<00:08,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -0.034137338399887085\n",
      "Validation Accuracy: 0.0\n",
      "Accuracy: 0.0\n",
      "Train loss: 6.957831382751465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  86%|████████▌ | 6/7 [00:24<00:04,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -0.041717078536748886\n",
      "Validation Accuracy: 0.125\n",
      "Accuracy: 0.125\n",
      "Train loss: 6.507972240447998\n",
      "Validation loss: -0.04975054785609245\n",
      "Validation Accuracy: 0.125\n",
      "Accuracy: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 7/7 [00:29<00:00,  4.18s/it]\n"
     ]
    }
   ],
   "source": [
    "def training():    \n",
    "    epochs = 7\n",
    "    max_grad_norm = 1.0\n",
    "    num_of_epoch = 0\n",
    "    accuracy_result = []\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        # TRAIN loop\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            # add batch to gpu\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_tgt_idxs, b_input_lus, b_input_frames, b_input_arg_idxs, b_input_args, b_input_masks = batch            \n",
    "            # forward pass\n",
    "            loss = model(b_input_ids, token_type_ids=None, tgt_idxs=b_input_tgt_idxs, \n",
    "                         lus=b_input_lus, frames=b_input_frames, arg_idxs=b_input_arg_idxs, args=b_input_args, attention_mask=b_input_masks)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            \n",
    "#             break\n",
    "\n",
    "        # print train loss per epoch\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        num_of_epoch += 1\n",
    "        model_path = model_dir+frarg_type+'arg_classifier-epoch-'+str(num_of_epoch)+'.pt'\n",
    "        torch.save(model, model_path)\n",
    "\n",
    "        # evaluation for each epoch\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        predictions , true_labels, scores, candis, all_frames = [], [], [], [], []\n",
    "        for batch in tst_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_tgt_idxs, b_lus, b_frames, b_arg_idxs, b_args, b_masks = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_eval_loss = model(b_input_ids, token_type_ids=None, tgt_idxs=b_tgt_idxs, \n",
    "                         lus=b_lus, frames=b_frames, arg_idxs=b_arg_idxs, attention_mask=b_masks)\n",
    "                logits = model(b_input_ids, token_type_ids=None, tgt_idxs=b_tgt_idxs, \n",
    "                         lus=b_lus, frames=b_frames, arg_idxs=b_arg_idxs, attention_mask=b_masks)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_args.to('cpu').numpy()          \n",
    "            masks = dataio.get_masks(b_frames, frargmap, num_label=len(arg2idx)).to(device)\n",
    "            for frame in b_frames:\n",
    "                candi_idx = frargmap[str(int(frame))]\n",
    "                candi = [idx2arg[c] for c in candi_idx]\n",
    "                candi_txt = ','.join(candi)\n",
    "                candi_txt = str(len(candi))+'\\t'+candi_txt\n",
    "                candis.append(candi_txt)\n",
    "                all_frames.append(idx2frame[int(frame)])\n",
    "            \n",
    "            for b_idx in range(len(logits)):\n",
    "                logit = logits[b_idx]\n",
    "                mask = masks[b_idx]\n",
    "                b_pred_idxs, b_pred_logits = [],[]\n",
    "                for fe_idx in range(len(mask)):\n",
    "                    if mask[fe_idx] > 0:\n",
    "                        b_pred_idxs.append(fe_idx)\n",
    "                        b_pred_logits.append(logit[0][fe_idx].item())\n",
    "                b_pred_idxs = torch.tensor(b_pred_idxs)\n",
    "                b_pred_logits = torch.tensor(b_pred_logits)\n",
    "                sm = nn.Softmax()\n",
    "                b_pred_logits = sm(b_pred_logits).view(1, -1)\n",
    "                score, indice = b_pred_logits.max(1)                \n",
    "                prediction = b_pred_idxs[indice]\n",
    "                predictions.append([int(prediction)])\n",
    "                score = float(score)\n",
    "                scores.append(score)\n",
    "            true_labels.append(label_ids)\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "#             break\n",
    "            \n",
    "        eval_loss = eval_loss/nb_eval_steps\n",
    "        print(\"Validation loss: {}\".format(eval_loss))\n",
    "        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "        pred_tags = [idx2arg[p_i] for p in predictions for p_i in p]\n",
    "        valid_tags = [idx2arg[l_ii] for l in true_labels for l_i in l for l_ii in l_i]        \n",
    "        \n",
    "        acc = accuracy_score(pred_tags, valid_tags)\n",
    "        accuracy_result.append(acc)\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "        \n",
    "        result_path = result_dir+frarg_type+str(version)+'.arg-classifier-'+str(num_of_epoch)+'.tsv'\n",
    "        with open(result_path,'w') as f:\n",
    "            line = 'gold' + '\\t' + 'prediction' + '\\t' + 'score' + '\\t' + 'input_frame' + '\\t' + 'sense_candidates'\n",
    "            f.write(line+'\\n')\n",
    "            for item in range(len(pred_tags)):\n",
    "                line = valid_tags[item] + '\\t' + pred_tags[item] + '\\t' + str(scores[item]) +'\\t'+ all_frames[item]+'\\t' + candis[item]\n",
    "                f.write(line+'\\n')\n",
    "        \n",
    "    accuracy_result_path = result_dir+frarg_type+str(version)+'.arg-classifier.accuracy'\n",
    "    with open(accuracy_result_path,'w') as f:\n",
    "        end_time = datetime.now()\n",
    "        running_time = 'running_ttime:'+str(end_time - start_time)\n",
    "        f.write(running_time+'\\n')\n",
    "        n = 0\n",
    "        for acc in accuracy_result:\n",
    "            f.write('epoch:'+str(n)+'\\t' + 'accuracy: '+str(acc)+'\\n')\n",
    "            n +=1\n",
    "\n",
    "training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
