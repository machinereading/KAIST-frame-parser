{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "### Korean FrameNet ###\n",
      "\t# contact: hahmyg@kaist, hahmyg@gmail.com #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from KAIST_frame_parser.src import dataio, etri\n",
    "from KAIST_frame_parser.src import targetid\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "from KAIST_frame_parser.koreanframenet.src import conll2textae\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class models():\n",
    "    def __init__(self, mode='parser', version=1.1, language='ko', model_dir=False):\n",
    "        self.version = version\n",
    "        self.mode = mode\n",
    "        if language == 'en':\n",
    "            self.framenet = 'fn'+str(version)\n",
    "        elif language == 'ko':\n",
    "            self.framenet = 'kfn'+str(version)\n",
    "            \n",
    "        if model_dir.endswith('/'):\n",
    "            pass\n",
    "        else:\n",
    "            model_dir += '/'\n",
    "        self.frameid_model_path = model_dir+self.framenet+'-frameid.pt'\n",
    "        self.arg_classifier_model_path = model_dir+self.framenet+'-arg_classifier.pt'\n",
    "        \n",
    "        self.bert_io = dataio.for_BERT(mode=self.mode, version=self.version)\n",
    "        self.frameid_model = torch.load(self.frameid_model_path)\n",
    "        self.arg_classifier_model = torch.load(self.arg_classifier_model_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            target_dir = os.path.dirname(os.path.abspath( __file__ ))\n",
    "        except:\n",
    "            target_dir = '.'\n",
    "        data_path = target_dir+'/koreanframenet/resource/info/'\n",
    "        with open(data_path+self.framenet+'_lu2idx.json','r') as f:\n",
    "            self.lu2idx = json.load(f)\n",
    "        with open(data_path+'fn1.7_frame2idx.json','r') as f:\n",
    "            self.frame2idx = json.load(f)      \n",
    "        with open(data_path+self.framenet+'_lufrmap.json','r') as f:\n",
    "            self.lufrmap = json.load(f)\n",
    "        with open(data_path+'fn1.7_fe2idx.json','r') as f:\n",
    "            self.arg2idx = json.load(f)\n",
    "        self.idx2frame = dict(zip(self.frame2idx.values(),self.frame2idx.keys()))\n",
    "        self.idx2lu = dict(zip(self.lu2idx.values(),self.lu2idx.keys()))\n",
    "        self.idx2arg = dict(zip(self.arg2idx.values(),self.arg2idx.keys()))\n",
    "        \n",
    "    def frame_identifier(self, tgt_data):\n",
    "        bert_inputs = self.bert_io.convert_to_bert_input_frameid(tgt_data)\n",
    "        dataloader = DataLoader(bert_inputs, sampler=None, batch_size=1)\n",
    "        \n",
    "        predictions, scores, candis = [], [], []\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_tgt_idxs, b_lus, b_masks = batch\n",
    "            with torch.no_grad():\n",
    "                logits = self.frameid_model(b_input_ids, token_type_ids=None, tgt_idxs=b_tgt_idxs, \n",
    "                                lus=b_lus, attention_mask=b_masks)\n",
    "            logits = logits.detach().cpu().numpy()                \n",
    "            masks = self.bert_io.get_masks(b_lus, model='frameid').to(device)\n",
    "            \n",
    "            for lu in b_lus:\n",
    "                candi_idx = self.lufrmap[str(int(lu))]\n",
    "                candi = [self.idx2frame[c] for c in candi_idx]\n",
    "                candi_txt = ','.join(candi)\n",
    "                candi_txt = str(len(candi))+'\\t'+candi_txt\n",
    "                candis.append(candi_txt)\n",
    "            for b_idx in range(len(logits)):\n",
    "                logit = logits[b_idx]\n",
    "                mask = masks[b_idx]\n",
    "                b_pred_idxs, b_pred_logits = [],[]\n",
    "                for fr_idx in range(len(mask)):\n",
    "                    if mask[fr_idx] > 0:\n",
    "                        b_pred_idxs.append(fr_idx)\n",
    "                        b_pred_logits.append(logit[0][fr_idx].item())\n",
    "                b_pred_idxs = torch.tensor(b_pred_idxs)\n",
    "                b_pred_logits = torch.tensor(b_pred_logits)\n",
    "                sm = nn.Softmax()\n",
    "                b_pred_logits = sm(b_pred_logits).view(1, -1)\n",
    "                score, indice = b_pred_logits.max(1)                \n",
    "                prediction = b_pred_idxs[indice]\n",
    "                predictions.append([int(prediction)])\n",
    "                score = float(score)\n",
    "                scores.append(score)\n",
    "        pred_tags = self.bert_io.idx2tag(predictions, model='frameid')       \n",
    "        conll, tuples = [],[]\n",
    "        for i in range(len(tgt_data)):\n",
    "            instance = tgt_data[i]\n",
    "            tokens, targets = instance[0], instance[1]\n",
    "            frames = ['_' for i in range(len(targets))]\n",
    "            for t in range(len(targets)):\n",
    "                if targets[t] != '_':\n",
    "                    frames[t] = pred_tags[i]\n",
    "            instance.append(frames)\n",
    "            conll.append(instance)\n",
    "            \n",
    "            tup = (pred_tags[i], scores[i])\n",
    "            tuples.append(tup)\n",
    "            \n",
    "        return conll, tuples\n",
    "    \n",
    "    def arg_classifier(self, fr_data, arg_tokens):\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        text = ' '.join(fr_data[0][0])\n",
    "        orig_tokens, bert_tokens, orig_to_tok_map = self.bert_io.bert_tokenizer_assign_to_last_token(text)     \n",
    "                \n",
    "        bert_inputs = self.bert_io.convert_to_bert_input_arg_classifier(fr_data)\n",
    "        dataloader = DataLoader(bert_inputs, sampler=None, batch_size=1)\n",
    "        predictions, scores = [],[]\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_tgt_idxs, b_lus, b_frames, b_masks = batch\n",
    "            \n",
    "            arg_word_idx = arg_tokens[-1]\n",
    "            arg_idx = [[orig_to_tok_map[arg_word_idx]]]\n",
    "            b_arg_idxs = torch.tensor(arg_idx)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = self.arg_classifier_model(b_input_ids, token_type_ids=None, tgt_idxs=b_tgt_idxs, \n",
    "                         lus=b_lus, frames=b_frames, arg_idxs=b_arg_idxs, attention_mask=b_masks)\n",
    "            logits = logits.detach().cpu().numpy()      \n",
    "            masks = self.bert_io.get_masks(b_frames, model='argid').to(device)\n",
    "            \n",
    "            for b_idx in range(len(logits)):\n",
    "                logit = logits[b_idx]\n",
    "                mask = masks[b_idx]\n",
    "                b_pred_idxs, b_pred_logits = [],[]\n",
    "                for fe_idx in range(len(mask)):\n",
    "                    if mask[fe_idx] > 0:\n",
    "                        b_pred_idxs.append(fe_idx)\n",
    "                        b_pred_logits.append(logit[0][fe_idx].item())\n",
    "                b_pred_idxs = torch.tensor(b_pred_idxs)\n",
    "                b_pred_logits = torch.tensor(b_pred_logits)\n",
    "                sm = nn.Softmax()\n",
    "                b_pred_logits = sm(b_pred_logits).view(1, -1)\n",
    "                score, indice = b_pred_logits.max(1)                \n",
    "                prediction = b_pred_idxs[indice]\n",
    "                predictions.append([int(prediction)])\n",
    "                score = float(score)\n",
    "                scores.append(score)\n",
    "            pred_tags = self.bert_io.idx2tag(predictions, model='argid')\n",
    "            result.append(pred_tags)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRLbasedParser():\n",
    "    def __init__(self, version=1.1, language='ko', model_dir=False):\n",
    "        try:\n",
    "            config_dir = os.path.dirname( os.path.abspath( __file__ ))\n",
    "        except:\n",
    "            config_dir = '.'\n",
    "        config_file = config_dir+'/config.json'\n",
    "        with open(config_file, 'r') as f:\n",
    "            config = json.load(f)\n",
    "            \n",
    "        self.version=version\n",
    "        self.language=language\n",
    "        self.model_dir=model_dir\n",
    "                \n",
    "        self.nlp_service = etri.etri(serviceType=config['nlp']['serviceType'], url=config['nlp']['url'], port=config['nlp']['port'])     \n",
    "        self.fn_models = models(mode='parser', version=self.version, language=self.language, model_dir=self.model_dir)\n",
    "        self.kkma = Kkma()\n",
    "\n",
    "        print('### SETTINGS')\n",
    "        print('\\t# FrameNet:', self.fn_models.framenet)\n",
    "        print('\\t# PARSER:', config['parser'])\n",
    "        print('\\t# MODEL_PATH:', model_dir)\n",
    "        print('\\t# (your frameid model should be:', self.fn_models.frameid_model_path)\n",
    "        print('\\t# (your argid model should be:', self.fn_models.arg_classifier_model_path)\n",
    "        \n",
    "    def doc2sents(self, text):\n",
    "        result = []\n",
    "        n = 0\n",
    "        sents = text.split('. ')\n",
    "        for sent in sents:\n",
    "            if len(sent) >0:\n",
    "                sent = sent+'.'\n",
    "                stringuri = 'test_0_'+str(n)\n",
    "                tup = (sent, stringuri)\n",
    "                result.append(tup)\n",
    "                n +=1\n",
    "        return result\n",
    "    \n",
    "    def result2triples(self, text, conll, tuples, stringuri):\n",
    "        triples = []\n",
    "        triple = (str(stringuri), 'nif:isString', text)\n",
    "        triples.append(triple)\n",
    "        # for target_id\n",
    "        if len(conll) > 0:\n",
    "            for i in range(len(conll)):\n",
    "                instance = conll[i]\n",
    "                tokens, targets, frames, args = instance[0], instance[1], instance[2], instance[3]\n",
    "                for tok in range(len(tokens)):\n",
    "                    if frames[tok] != '_':\n",
    "                        frame = 'frame:'+frames[tok]\n",
    "                        lu = targets[tok]\n",
    "                triple = (frame, 'frdf:provinence', str(stringuri))\n",
    "                triples.append(triple)\n",
    "                triple = (frame, 'frdf:lu', lu)\n",
    "                triples.append(triple)\n",
    "                triple = (frame, 'frdf:score', str(tuples[i][1]))\n",
    "                triples.append(triple)\n",
    "\n",
    "                #args to triples\n",
    "                for idx in range(len(args)):\n",
    "                    arg_tag = args[idx]\n",
    "                    arg_tokens = []\n",
    "                    if arg_tag.startswith('B'):\n",
    "                        fe_tag = arg_tag.split('-')[1]\n",
    "                        arg_tokens.append(tokens[idx])\n",
    "                        next_idx = idx + 1\n",
    "                        while next_idx < len(args) and args[next_idx] == 'I-'+fe_tag:\n",
    "                            arg_tokens.append(tokens[next_idx])\n",
    "                            next_idx +=1\n",
    "                        arg_text = ' '.join(arg_tokens)\n",
    "                        triple = (frame, 'arg:'+fe_tag, arg_text)\n",
    "                        triples.append(triple)\n",
    "\n",
    "        return triples\n",
    "    \n",
    "    def arg_identifier(self, conll):\n",
    "        text = ' '.join(conll[0][0])\n",
    "        pas = SRLbasedParser.sent2pa(self, text)\n",
    "\n",
    "        result = []\n",
    "        for anno in conll:\n",
    "            args = ['O' for i in range(len(anno[1]))]\n",
    "            for idx in range(len(anno[1])):\n",
    "                if anno[1][idx] != '_':\n",
    "                    lu = anno[1][idx]\n",
    "                    frame = anno[2][idx]\n",
    "                    target_idx = idx\n",
    "            for pa in pas:\n",
    "                if target_idx == pa['predicate']['id']:                \n",
    "                    for argument in pa['arguments']:\n",
    "                        arg_tokens = argument['tokens']\n",
    "\n",
    "                        pred = self.fn_models.arg_classifier([anno], arg_tokens)[0][0]\n",
    "                        for arg_token_idx in range(len(arg_tokens)):\n",
    "                            arg_token = arg_tokens[arg_token_idx]\n",
    "                            if arg_token_idx == 0:\n",
    "                                fe = 'B-'+pred\n",
    "                            else:\n",
    "                                fe = 'I-'+pred\n",
    "                            args[arg_token] = fe                \n",
    "\n",
    "            new_anno = anno\n",
    "            new_anno.append(args)\n",
    "            result.append(new_anno)\n",
    "        return result\n",
    "    \n",
    "    def sent2pa(self, text):\n",
    "        nlp = self.nlp_service.getETRI(text)    \n",
    "        conll_2009 = self.nlp_service.getETRI_CoNLL2009(nlp)\n",
    "        predicate_argument = self.nlp_service.phrase_parser(conll_2009, nlp)\n",
    "        return predicate_argument\n",
    "    \n",
    "    def parser(self, data, sentence_id='stringurl:null'):\n",
    "        input_data = dataio.preprocessor(data)\n",
    "        text = data\n",
    "\n",
    "        tgt_data = targetid.baseline(input_data)\n",
    "        fid_data, fid_result  = self.fn_models.frame_identifier(tgt_data)\n",
    "        if len(fid_data) > 0:\n",
    "            argid_data = SRLbasedParser.arg_identifier(self, fid_data)\n",
    "        else:\n",
    "            argid_data = []\n",
    "\n",
    "        framegraph = SRLbasedParser.result2triples(self, text, argid_data, fid_result, str(sentence_id))\n",
    "        textae = conll2textae.get_textae(argid_data)\n",
    "\n",
    "        result = {}\n",
    "        result['graph'] = framegraph\n",
    "        result['textae'] = textae\n",
    "        result['conll'] = argid_data\n",
    "        return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parser():\n",
    "    language = 'ko'\n",
    "    version = 1.1\n",
    "    model_dir = '/disk_4/resource/models/'\n",
    "    parser = SRLbasedParser(language='ko', version=version, model_dir=model_dir)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SETTINGS\n",
      "\t# FrameNet: kfn1.1\n",
      "\t# PARSER: srl-based\n",
      "\t# MODEL_PATH: /disk_4/resource/models/\n",
      "\t# (your frameid model should be: /disk_4/resource/models/kfn1.1-frameid.pt\n",
      "\t# (your argid model should be: /disk_4/resource/models/kfn1.1-arg_classifier.pt\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8159265834e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'graph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-8159265834e9>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'나는 밥을 먹었다.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstringuri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test:offset_0_53'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstringuri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'graph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-973f989e56fb>\u001b[0m in \u001b[0;36mparser\u001b[0;34m(self, data, sentence_id)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0margid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSRLbasedParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_identifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mframegraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSRLbasedParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult2triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfid_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-973f989e56fb>\u001b[0m in \u001b[0;36marg_identifier\u001b[0;34m(self, conll)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marg_identifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mpas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSRLbasedParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent2pa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    parser = load_parser()\n",
    "    text = '헤밍웨이는 1899년 7월 21일 미국 일리노이에서 태어났고, 62세에 자살로 사망했다.'\n",
    "    text = '나는 밥을 먹었다.'\n",
    "    stringuri = 'test:offset_0_53'\n",
    "    parsed = parser.parser(text, sentence_id=stringuri)\n",
    "    pprint(parsed['graph'])\n",
    "    \n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
