{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from tqdm import tqdm, trange\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "from KAIST_frame_parser.src import dataio\n",
    "from KAIST_frame_parser.src.fn_modeling import BertForFrameIdentification\n",
    "from KAIST_frame_parser.src.fn_modeling import BertForArgClassification\n",
    "from KAIST_frame_parser.koreanframenet import koreanframenet\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg_classifier():\n",
    "    def __init__(self, language='ko', version=1.1):\n",
    "        self.language = language\n",
    "        self.version = version\n",
    "        if self.language == 'en':\n",
    "            self.framenet = 'fn'+str(version)\n",
    "        elif self.language == 'ko':\n",
    "            self.framenet = 'kfn'+str(version)            \n",
    "        print('### SETINGS')\n",
    "        print('\\t# FrameNet:', self.framenet)        \n",
    "        if self.language == 'ko':\n",
    "            kfn = koreanframenet.interface(version=version)\n",
    "            self.trn, self.dev, self.tst = kfn.load_data()            \n",
    "        try:\n",
    "            target_dir = os.path.dirname(os.path.abspath( __file__ ))\n",
    "        except:\n",
    "            target_dir = '.'\n",
    "        data_path = target_dir+'/../koreanframenet/resource/info/'\n",
    "        with open(data_path+self.framenet+'_lu2idx.json','r') as f:\n",
    "            self.lu2idx = json.load(f)\n",
    "        with open(data_path+'fn1.7_frame2idx.json','r') as f:\n",
    "            self.frame2idx = json.load(f)      \n",
    "        with open(data_path+self.framenet+'_lufrmap.json','r') as f:\n",
    "            self.lufrmap = json.load(f)\n",
    "        with open(data_path+'fn1.7_fe2idx.json','r') as f:\n",
    "            self.arg2idx = json.load(f)\n",
    "        self.idx2frame = dict(zip(self.frame2idx.values(),self.frame2idx.keys()))\n",
    "        self.idx2lu = dict(zip(self.lu2idx.values(),self.lu2idx.keys()))\n",
    "        self.idx2arg = dict(zip(self.arg2idx.values(),self.arg2idx.keys()))\n",
    "        \n",
    "        with open(data_path+'fn1.7_frargmap.json','r') as f:\n",
    "            self.frargmap = json.load(f)\n",
    "            \n",
    "    def flat_accuracy(self, preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "            \n",
    "    def data2argdata(self, data):\n",
    "        result = []\n",
    "        for i in data:\n",
    "            tokens, lus, frames, args = i[0],i[1],i[2],i[3]\n",
    "            for idx in range(len(args)):\n",
    "                arg_tag = args[idx]\n",
    "                if arg_tag.startswith('B'):\n",
    "                    new_args = ['O' for i in range(len(tokens))]                \n",
    "                    fe_tag = arg_tag.split('-')[1]\n",
    "                    next_idx = idx + 1\n",
    "                    while next_idx < len(args) and args[next_idx] == 'I-'+fe_tag:\n",
    "                        next_idx +=1\n",
    "                    new_args[next_idx-1] = fe_tag\n",
    "                    new_sent = []\n",
    "                    new_sent.append(tokens)\n",
    "                    new_sent.append(lus)\n",
    "                    new_sent.append(frames)\n",
    "                    new_sent.append(new_args)\n",
    "                    result.append(new_sent)\n",
    "        return result\n",
    "    \n",
    "    def gen_bert_input_representation(self, fn_data, MAX_LEN=256, batch_size=8):\n",
    "        bert_io = dataio.for_BERT(mode='training', version=self.version)\n",
    "        data = bert_io.convert_to_bert_input_arg_classifier(fn_data)\n",
    "        sampler = RandomSampler(fn_data)\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)        \n",
    "        return data, sampler, dataloader\n",
    "        \n",
    "    def train(self, model_dir='.', trn=False, dev=False, MAX_LEN = 256, batch_size = 8, epoch=4):\n",
    "        model_path = model_dir+'/'+self.framenet+'-arg_classifier.pt'\n",
    "        print('your model would be saved at', model_path)\n",
    "        \n",
    "        # load BERT model for arg-classification\n",
    "        model = BertForArgClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels = len(self.arg2idx), num_lus = len(self.lu2idx), num_frames = len(self.frame2idx), ludim = 64, framedim = 100, frargmap=self.frargmap)\n",
    "        model.cuda();\n",
    "        \n",
    "        # trn to arg-granularity data\n",
    "        trn = arg_classifier.data2argdata(self, trn)\n",
    "        \n",
    "        # gen BERT input representations            \n",
    "        trn_data, trn_sampler, trn_dataloader = arg_classifier.gen_bert_input_representation(self, trn, MAX_LEN=256, batch_size=8)\n",
    "        \n",
    "        # load optimizer\n",
    "        FULL_FINETUNING = True\n",
    "        if FULL_FINETUNING:\n",
    "            param_optimizer = list(model.named_parameters())\n",
    "            no_decay = ['bias', 'gamma', 'beta']\n",
    "            optimizer_grouped_parameters = [\n",
    "                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                 'weight_decay_rate': 0.01},\n",
    "                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                 'weight_decay_rate': 0.0}\n",
    "            ]\n",
    "        else:\n",
    "            param_optimizer = list(model.classifier.named_parameters()) \n",
    "            optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "        optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "        \n",
    "        # train\n",
    "        epochs = epoch\n",
    "        max_grad_norm = 1.0\n",
    "        num_of_epoch = 0\n",
    "        accuracy_result = []\n",
    "        for _ in trange(epochs, desc=\"Epoch\"):\n",
    "            # TRAIN loop\n",
    "            model.train()\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            for step, batch in enumerate(trn_dataloader):\n",
    "                # add batch to gpu\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_tgt_idxs, b_input_lus, b_input_frames, b_input_arg_idxs, b_input_args, b_input_masks = batch            \n",
    "                # forward pass\n",
    "                loss = model(b_input_ids, token_type_ids=None, tgt_idxs=b_input_tgt_idxs, \n",
    "                             lus=b_input_lus, frames=b_input_frames, arg_idxs=b_input_arg_idxs, args=b_input_args, attention_mask=b_input_masks)\n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "                # track train loss\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += b_input_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "                # gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "                # update parameters\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "\n",
    "            # print train loss per epoch\n",
    "            print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "            torch.save(model, model_path)\n",
    "            num_of_epoch += 1\n",
    "        print('...training is done')\n",
    "        end_time = datetime.now()\n",
    "        running_time = 'running_ttime:'+str(end_time - start_time)\n",
    "        print('running_time:', running_time)          \n",
    "        \n",
    "    def test(self, tst=False, model_dir='.', MAX_LEN = 256, batch_size = 8):\n",
    "        model_path = model_dir+'/'+self.framenet+'-arg_classifier.pt'\n",
    "        print('your model is', model_path)\n",
    "        model = torch.load(model_path)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # trn to arg-granularity data\n",
    "        tst = arg_classifier.data2argdata(self, tst)\n",
    "        \n",
    "        # gen BERT input representations            \n",
    "        tst_data, tst_sampler, tst_dataloader = arg_classifier.gen_bert_input_representation(self, tst, MAX_LEN=256, batch_size=8)\n",
    "        \n",
    "        \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        predictions , true_labels, scores, candis, all_frames = [], [], [], [], []\n",
    "        for batch in tst_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_tgt_idxs, b_lus, b_frames, b_arg_idxs, b_args, b_masks = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_eval_loss = model(b_input_ids, token_type_ids=None, tgt_idxs=b_tgt_idxs, \n",
    "                         lus=b_lus, frames=b_frames, arg_idxs=b_arg_idxs, attention_mask=b_masks)\n",
    "                logits = model(b_input_ids, token_type_ids=None, tgt_idxs=b_tgt_idxs, \n",
    "                         lus=b_lus, frames=b_frames, arg_idxs=b_arg_idxs, attention_mask=b_masks)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_args.to('cpu').numpy()          \n",
    "            masks = dataio.get_masks(b_frames, self.frargmap, num_label=len(self.arg2idx)).to(device)\n",
    "            for frame in b_frames:\n",
    "                candi_idx = self.frargmap[str(int(frame))]\n",
    "                candi = [self.idx2arg[c] for c in candi_idx]\n",
    "                candi_txt = ','.join(candi)\n",
    "                candi_txt = str(len(candi))+'\\t'+candi_txt\n",
    "                candis.append(candi_txt)\n",
    "                all_frames.append(self.idx2frame[int(frame)])\n",
    "            \n",
    "            for b_idx in range(len(logits)):\n",
    "                logit = logits[b_idx]\n",
    "                mask = masks[b_idx]\n",
    "                b_pred_idxs, b_pred_logits = [],[]\n",
    "                for fe_idx in range(len(mask)):\n",
    "                    if mask[fe_idx] > 0:\n",
    "                        b_pred_idxs.append(fe_idx)\n",
    "                        b_pred_logits.append(logit[0][fe_idx].item())\n",
    "                b_pred_idxs = torch.tensor(b_pred_idxs)\n",
    "                b_pred_logits = torch.tensor(b_pred_logits)\n",
    "                sm = nn.Softmax()\n",
    "                b_pred_logits = sm(b_pred_logits).view(1, -1)\n",
    "                score, indice = b_pred_logits.max(1)                \n",
    "                prediction = b_pred_idxs[indice]\n",
    "                predictions.append([int(prediction)])\n",
    "                score = float(score)\n",
    "                scores.append(score)\n",
    "            true_labels.append(label_ids)\n",
    "            tmp_eval_accuracy = arg_classifier.flat_accuracy(self, logits, label_ids)\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "        pred_tags = [self.idx2arg[p_i] for p in predictions for p_i in p]\n",
    "        valid_tags = [self.idx2arg[l_ii] for l in true_labels for l_i in l for l_ii in l_i]        \n",
    "        \n",
    "        acc = accuracy_score(pred_tags, valid_tags)\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "        \n",
    "        return acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
